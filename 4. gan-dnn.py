# Stock Price Prediction with Keras RNN
# July 5, 2019
# Sung Kyu Lim
# Georgia Institute of Technology
# limsk@ece.gatech.edu


# import packages
import numpy as np
import matplotlib.pyplot as plt
import os
from keras import models
from keras.layers import Dense, Conv1D, Reshape, Flatten, Lambda
from keras.optimizers import Adam
from keras import backend as K


# create directories
OUT_DIR = "./output/"
if not os.path.exists(OUT_DIR):
    os.makedirs(OUT_DIR)


# global constants and hyper-parameters
NUM_DATA = 1000
MY_BATCH = 1
MY_STAGE = 10
MY_EPOCH = 100
MY_TEST = 20

D_SIZE = 50
G_SIZE = 50
D_LEARN = 1
G_LEARN = 5

MY_MU = 10
MY_SIGMA = 1.0
MY_ADAM = Adam(lr = 0.0002, beta_1 = 0.9, beta_2 = 0.999)


# we use the same compilation setting for all models
def model_compile(model):
    return model.compile(loss = 'binary_crossentropy', optimizer = MY_ADAM, 
            metrics=['accuracy'])


    ####################
    # DATABASE SETTING #
    ####################


# we do NOT use a separate DB
# instead we build random data on-the-fly
# our goal is to learn the distribution itself

# random samples with mean = mu and stddev = sigma
def real_sample():
    return np.random.normal(MY_MU, MY_SIGMA, (MY_BATCH, NUM_DATA))


# random samples from a uniform distribution over [0, 1).
def in_sample():
    return np.random.rand(MY_BATCH, NUM_DATA)



    ##################
    # MODEL BUILDING #
    ##################


# DNN discriminator definition
def gen_D():
    D = models.Sequential()

    # first dense layer with 50 neurons
    # input shape is (100,)
    D.add(Dense(D_SIZE, activation = 'relu', input_shape = (NUM_DATA,)))

    # second dense layer 
    D.add(Dense(D_SIZE, activation = 'relu'))
    
    # third dense layer
    D.add(Dense(D_SIZE, activation = 'relu'))

    # we need a single output for classification: real or fake
    D.add(Dense(1, activation = 'sigmoid'))

    # compile the model with common setting
    model_compile(D)

    print('\n== DISCRIMINATOR MODEL DETAILS ==')
    D.summary()
    return D


# CNN generator definition
def gen_G():

    G = models.Sequential()

    # we use CNN for generator
    # CNN needs channel info, so we reshape the CNN input to (100, 1)
    G.add(Reshape((NUM_DATA, 1), input_shape = (NUM_DATA,)))

    # first 1-dimensional CNN layer
    # kernel size is 1
    # total parameter count formula = 
    # (filter_height * filter_width * input_channels + 1) * output_channel
    # (1 * 1 * 1 + 1) * 50
    G.add(Conv1D(G_SIZE, 1, activation = 'relu'))

    # second 1-dimensional CNN layer
    # (1 * 1 * 50 + 1) * 50
    G.add(Conv1D(G_SIZE, 1, activation = 'sigmoid'))

    # third 1-dimensional CNN layer
    # (1 * 1 * 50 + 1) * 1
    # the final output is a single number
    G.add(Conv1D(1, 1))

    # flatten it to one dimension
    G.add(Flatten())

    # compile the model with common setting
    model_compile(G)

    print('\n== GENERTOR MODEL DETAILS ==')
    G.summary()
    return G


# GAN definition
def make_GAN(dis, gen):
    GAN = models.Sequential()
    
    # add the generator
    GAN.add(gen)

    # add the discriminator
    GAN.add(dis)

    # discriminator needs 
    dis.trainable = False
    model_compile(GAN)

    print('\n== GAN MODEL DETAILS ==')
    GAN.summary()
    
    return GAN



    ##################
    # MODEL TRAINING #
    ##################


# training discriminator
# input: real and fake images combined (# batch X 2, data size)
# output: the labels that say that real data is real
#         plus the fake data is fake (# batch X 2)
def D_train_on_batch(real, gen, dis):

    # real and fake data merged into a single array
    # two (1, 100) arrays concatenated, we get (2, 100)
    X = np.concatenate([real, gen], axis = 0)


    # build the output (= label) array
    # build a single array with 1's and 0's
    # the number of 1's is the same as the number of batch
    # the number of 0's is the same as the number of batch
    # so the label array becomes [1 0]
    # 1 means real, 0 fake
    y = np.array([1] * real.shape[0] + [0] * gen.shape[0])
    # print('Discriminator training input and output shape:', X.shape, y.shape)
    
    # Keras train_on_batch
    # (input, label)
    dis.train_on_batch(X, y)


# discriminator training
def train_D(gan, dis, gen):

    # real data for discriminator training
    # this data is generated randomly
    # shape is (batch, 100)
    real = real_sample()
    
    # input data for discriminator training
    # this data is generated randomly
    # shape is (batch, 100)
    Z = in_sample()

    # fake data generated by the generator
    # we use the input data
    # shape is (batch, 100)
    fake = gen.predict(Z)

    dis.trainable = True
    D_train_on_batch(real, fake, dis)


# we train the generator with input data 
# and the labels that say that the data is real
# input shape is (# batch, data size)
# output shape is (# batch,)
def GAN_train_on_batch(Z, gan):

    # build the true label array
    # we get [1]
    y = np.array([1] * Z.shape[0])
    # print('Generator training input and output shape:', Z.shape, y.shape)


    # Keras train_on_batch
    # (input, label)
    gan.train_on_batch(Z, y)



# generator training
def train_GAN(gan, dis, gen):
    # input data for generator training
    # this data is generated randomly again
    Z = in_sample()


    # we do not train discriminator during generator training
    dis.trainable = False
    GAN_train_on_batch(Z, gan)


# run epochs
def train_epochs(gan, dis, gen):
    for e in range(MY_EPOCH):

        # number of times we train discriminator in each epoc (1 currently)
        for i in range(D_LEARN):
            train_D(gan, dis, gen)

        # number of times we train generator in each epoch (5 currently)
        for i in range(G_LEARN):
            train_GAN(gan, dis, gen)


    ####################
    # MODEL EVALUATION #
    ####################


# we generate test data and use them for prediction
# test data input shape is (# test data, data size)
# fake data shape is (# test data, data size)
def test(data):

    # generate test data input (random)
    # n_test is the number of test data
    Z = np.random.rand(MY_TEST, NUM_DATA)


    # our generator produces fake data using the test input
    fake = gen.predict(Z)
    # print('Test data input and fake shape:', Z.shape, fake.shape)

    return fake, Z


# show histograms
# real data, fake data, input data
def show_hist(Real, Gen, Z):
    plt.hist(Real.reshape(-1), histtype = 'step', label = 'Real')
    plt.hist(Gen.reshape(-1), histtype = 'step', label = 'Generated')
    plt.hist(Z.reshape(-1), histtype = 'step', label = 'Input')
    plt.legend(loc = 0)


# main routine to orchestrate the test process
def test_and_show():
    # generate random test input and their fake data
    fake, Z = test(MY_TEST)

    # generate random real data
    real = np.random.normal(MY_MU, MY_SIGMA, (MY_TEST, NUM_DATA))


    # no need to use discriminator
    # we just compare the distribution between the real and fake
    # we want them to be similar
    show_hist(real, fake, Z)

    print('        Real: mean = {:.2f}'.format(np.mean(real)), 
            ', std-dev = {:.2f}'.format(np.std(real)))
    print('        Fake: mean = {:.2f}'.format(np.mean(fake)), 
            ', std-dev = {:.2f}'.format(np.std(fake)))


# train and test GAN
def run(gan, dis, gen):

    for i in range(MY_STAGE):
        print('\nStage', i, '(Epoch: {})'.format(i * MY_EPOCH))

        train_epochs(gan, dis, gen)
        test_and_show()

        path = "output/chap4-img-{}".format(i)
        plt.savefig(path)
        plt.close()


# build and run GAN 
dis = gen_D()
gen = gen_G()
GAN = make_GAN(dis, gen)
run(GAN, dis, gen)
